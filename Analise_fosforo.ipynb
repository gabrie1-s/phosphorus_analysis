{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrie1-s/phosphorus_analysis/blob/main/Analise_fosforo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80wbHFyh27Y8"
      },
      "source": [
        "#1. Importa√ß√£o das bibliotecas e leitura dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jkz8PjyiQvC1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install Bayesian-Optimization\n",
        "!pip install xgboost\n",
        "!pip install ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l1zihLe9RKbY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kj1_RVUPfWbe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import pdb\n",
        "import ray\n",
        "import keras\n",
        "import xgboost\n",
        "import tensorflow\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import load_model\n",
        "from bayes_opt import BayesianOptimization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6FFRSFJRVqY"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bVQdxJiRMVT"
      },
      "outputs": [],
      "source": [
        "pentecoste = pd.read_excel(\"/content/drive/MyDrive/CNN_Chagas/fosforo/Analise_fosforo.xlsx\", sheet_name=\"Pentecoste\")\n",
        "acarape = pd.read_excel(\"/content/drive/MyDrive/CNN_Chagas/fosforo/Analise_fosforo.xlsx\", sheet_name=\"Acarape do meio\")\n",
        "aracoiaba = pd.read_excel(\"/content/drive/MyDrive/CNN_Chagas/fosforo/Analise_fosforo.xlsx\", sheet_name=\"Aracoiaba\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdOGLLQKSr_s"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_gk5PZlSQe4"
      },
      "outputs": [],
      "source": [
        "def prepare_data(df):\n",
        "  df.columns = df.iloc[0]\n",
        "  df = df.iloc[1:, :]\n",
        "  df = df[[\"P medido (mg/L)\",\"TOA  B2\", \"TOA B3\"]]\n",
        "  df.dropna(inplace=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUpzF56VTyBG"
      },
      "outputs": [],
      "source": [
        "pentecoste = prepare_data(pentecoste.copy())\n",
        "pentecoste = pentecoste.iloc[:, 1:]\n",
        "\n",
        "acarape = prepare_data(acarape.copy())\n",
        "aracoiaba = prepare_data(aracoiaba.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-bQVXoHecIv"
      },
      "source": [
        "#2. An√°lise explorat√≥ria e preprocessamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnbCrvYIfaLV"
      },
      "outputs": [],
      "source": [
        "dataset = pd.concat([pentecoste, acarape, aracoiaba])\n",
        "dataset = dataset.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNy7A5kHGsvp"
      },
      "outputs": [],
      "source": [
        "dataset['B3/B2'] = dataset['TOA B3'] / dataset['TOA  B2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPMHuinmqcR-"
      },
      "outputs": [],
      "source": [
        "dataset.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsKFmRjXqZH2"
      },
      "source": [
        "##2.1. Breve observa√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91xQua44q4tU"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(dataset, corner=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn5ZHS0ur0D5"
      },
      "source": [
        "Observe no conjunto de gr√°ficos de dispers√£o acima que a vari√°vel B2 parece estar altamente correlacionada com a vari√°vel B3. Notamos isso pelo comportamento quase linear dos pontos do gr√°fico 'TOA B2' x 'TOA B3'. Em contrapartida, n√£o parece haver rel√ß√£o entre B3 e 'P medido' ou B2 e 'P medido'.  Al√©m disso, no mapa de calor fica evidente a maior correla√ß√£o da vari√°vel P com a raz√£o B3/B2. Embora tal correla√ß√£o n√£o seja significativa, √© consideravelmente maior que as demais correla√ß√µes da vari√°vel P."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-SS0DeLrwWn"
      },
      "outputs": [],
      "source": [
        "#Sem transforma√ß√£o\n",
        "ax = sns.heatmap(dataset.corr(), annot=True, vmin=-1, vmax=1)\n",
        "ax.set_title(\"Mapa de calor da correla√ß√£o entre as vari√°veis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX_29-SPscYc"
      },
      "source": [
        "No mapa de calor acima, podemos visualizar as correla√ß√µes entre as vari√°veis. Vemos que as informa√ß√µes do mapa de calor apenas confirmam o que foi dito anteriormente: as vari√°veis B2 e B3 s√£o altamente correlacionadas entre si e ambas tem pouca (ou nenhuma) correla√ß√£o com a vari√°vel P. Al√©m disso, no mapa de calor fica evidente a maior correla√ß√£o da vari√°vel P com a raz√£o B3/B2. Embora tal correla√ß√£o n√£o seja significativa, √© consideravelmente maior que as demais correla√ß√µes da vari√°vel P.\n",
        "\n",
        "$\\textbf{Nota}$ : a correla√ß√£o entre duas vari√°veis $x$ e $y$ √© dada por:\n",
        "\\begin{equation}\n",
        "  \\rho_{x, y} = \\frac{covariance(x, y)}{\\sigma_{x} \\cdot \\sigma_{y}}\n",
        "\\end{equation}\n",
        "\n",
        "O valor de $\\rho_{x,y}$ √© tal que: $-1 \\leq \\rho_{x,y} \\leq 1$. Quanto maior o m√≥dulo de $\\rho_{x,y}$ maior a correla√ß√£o entre $x$ e $y$. Quanto mais pr√≥ximo $\\rho_{x,y}$ for de zero, menor a correla√ß√£o.\n",
        "\n",
        "Como o m√≥dulo da correla B2 e B3, significa que a vari√°vel B2 acresecnta muito pouca informa√ß√£o a B3 e vice-versa. Em casos de correla√ß√£o t√£o elevada, normalmente, elimina-se uma das vari√°veis ou substitui-se as vari√°veis por uma combina√ß√£o linear de ambas. Entretanto, como desconhe√ßo o problema (s√≥ me foi dito que deveria tentar predizer P com base, apenas, em B2 e B3 üòÖ) decidi manter ambas as vari√°veis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIxNH8HVqcR_"
      },
      "outputs": [],
      "source": [
        "b2 = np.array(dataset[dataset.columns[1]])\n",
        "b3 = np.array(dataset[dataset.columns[2]])\n",
        "\n",
        "b2 = (b2-b2.min())/((b2.max()-b2.min()))\n",
        "b3 = (b3-b3.min())/((b3.max()-b3.min()))\n",
        "\n",
        "b3 = [x for _, x in sorted(zip(b2, b3))]\n",
        "b2 = sorted(b2)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,6)\n",
        "\n",
        "plt.scatter(range(len(b2)), b2)\n",
        "plt.scatter(range(len(b2)), b3)\n",
        "plt.title('B2 x B3 comparison')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Samples')\n",
        "plt.legend(['B2', 'B3'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaSUQ2Anxlkv"
      },
      "source": [
        "Como forma de visualizar melhor essa correla√ß√£o, fizmeos o plot acima, perceba que os valores de B3 seguem a mesma tend√™ncia de crescimento de B2.\n",
        "\n",
        "**OBS**.: Para plotarmos o gr√°fico acima, ordenamos os valores de B2 e reposicionamos os valores de B3 de forma a n√£o alterar os pares (B2, B3) do conjunto de dados original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTE8GZgy9TJT"
      },
      "source": [
        "##2.2. An√°lise da skewness (obliquidade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jphro6Tx3MiK"
      },
      "outputs": [],
      "source": [
        "dataset.hist(\n",
        "    figsize=(20, 8),\n",
        "    grid = False,\n",
        "    rwidth = 0.8,\n",
        "    bins = 100\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fXKzEEu6KPr"
      },
      "outputs": [],
      "source": [
        "dataset.skew()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLtU6hDt7gOC"
      },
      "source": [
        "Geralmente, skewnesses (obliquidades) maiores que 1 ou menores que -1 s√£o consideradas elevadas. Testaremos dois m√©todos para reduzir o valor da skewness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e_nBRpW7yGG"
      },
      "source": [
        "###2.2.1. M√©todo do Box-cox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Am7UxuZy0t2"
      },
      "source": [
        "Essa trasnforma√ß√£o √© dad por:\n",
        "\n",
        "\\begin{equation}\n",
        "      y(\\lambda) =\n",
        "      \\begin{cases}\n",
        "          \\frac{y^\\lambda - 1}{\\lambda} & \\text{se $\\lambda \\neq 0$}\\\\\n",
        "          log(y) & \\text{se $\\lambda = 0$}\n",
        "      \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "A ideia √© encontrar um valor de $\\lambda$ (um escalar) que mais aproxima a distribui√ß√£o $y(\\lambda)$ de uma distribui√ß√£o normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTfWj-RO6_hA"
      },
      "outputs": [],
      "source": [
        "test_data = dataset.copy()\n",
        "lbs = []\n",
        "\n",
        "for col in test_data.columns[:-1]:\n",
        "  test_data[col], l = stats.boxcox(test_data[col])[0:2]\n",
        "  lbs.append(l)\n",
        "test_data.hist(\n",
        "    figsize=(20, 8),\n",
        "    grid = False,\n",
        "    rwidth = 0.8,\n",
        "    bins = 100\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwnk2C_J7TPr"
      },
      "outputs": [],
      "source": [
        "test_data.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CPolphFonnL"
      },
      "outputs": [],
      "source": [
        "test_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoQf-KoFokf9"
      },
      "outputs": [],
      "source": [
        "lbs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdJpwUP5pKhq"
      },
      "source": [
        "Cada valor na lista acima refere-se ao valor de $\\lambda$ de cada vari√°vel: 'P medido', 'TOA  B2', 'TOA B3', respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US6IXwtx8Kcn"
      },
      "source": [
        "###2.2.2. M√©todo de Yeo-Johnson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPu9JFyozsQw"
      },
      "source": [
        "Essa trasnforma√ß√£o √© dad por:\n",
        "\n",
        "\\begin{equation}\n",
        "      \\psi(y, \\lambda) =\n",
        "      \\begin{cases}\n",
        "          \\frac{(y+1)^\\lambda - 1}{\\lambda} & \\text{se $y \\geq 0 \\ e \\ \\lambda \\neq 0$}\\\\\n",
        "          log(y+1) & \\text{se $y \\geq 0 \\ e \\ \\lambda = 0$}\\\\\n",
        "          -\\frac{(-y+1)^{2-\\lambda} - 1}{2 - \\lambda} & \\text{se $y < 0 \\ e \\ \\lambda \\neq 2$}\\\\\n",
        "          -log(-y+1) & \\text{se $y < 2 \\ e \\ \\lambda = 2$}\\\\\n",
        "      \\end{cases}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYuvJNnk7ae7"
      },
      "outputs": [],
      "source": [
        "test_data = dataset.copy()\n",
        "lbs = []\n",
        "\n",
        "for col in test_data.columns[:-1]:\n",
        "  test_data[col], l = stats.yeojohnson(test_data[col])[0:2]\n",
        "  lbs.append(l)\n",
        "\n",
        "test_data.hist(\n",
        "    figsize=(20, 8),\n",
        "    grid = False,\n",
        "    rwidth = 0.8,\n",
        "    bins = 100\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSUHF-zN8lv5"
      },
      "outputs": [],
      "source": [
        "test_data.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy1oHN1io67Z"
      },
      "outputs": [],
      "source": [
        "print(test_data.columns)\n",
        "print(lbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJQn0n1vpx1S"
      },
      "source": [
        "Cada valor na lista acima refere-se ao valor de $\\lambda$ de cada vari√°vel: 'P medido', 'TOA  B2', 'TOA B3', respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlIPzo_R8spA"
      },
      "source": [
        "Os resultados foram semelhantes, mas, no caso do Box-cox, a redu√ß√£o da skewness da vari√°vel 'P medido' foi bem mais significativa, logo, optaremos por este m√©todo.\n",
        "\n",
        "Note que, quando aplicarmos essa transforma√ß√£o, as unidades das vari√°veis tamb√©m ser√£o alteradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZoUhTy59Gt0"
      },
      "outputs": [],
      "source": [
        "for col in dataset.columns[:-1]:\n",
        "  dataset[col] = stats.boxcox(dataset[col])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLxbJNkc9ZsT"
      },
      "source": [
        "##2.3. An√°lise de outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8WR7mr19jMg"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,6)\n",
        "fig, axs = plt.subplots(1, 4)\n",
        "\n",
        "i=0\n",
        "for col in dataset:\n",
        "  axs[i].set_title(col)\n",
        "  axs[i].boxplot(dataset[col])\n",
        "  i +=1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_UaUsI9Nqe"
      },
      "source": [
        "Ap√≥s o tratamento da skewness quase n√£o observamos a presen√ßa de outliers. Devido a isso, optaremos por mant√™-los."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw_pUqEU_Xvp"
      },
      "outputs": [],
      "source": [
        "'''def plot_boxplot(df, ft):\n",
        "  df.boxplot(column = [ft])\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "\n",
        "def outliers(df, ft):\n",
        "  Q1 = df[ft].quantile(0.25)\n",
        "  Q3 = df[ft].quantile(0.75)\n",
        "  I = Q3 - Q1\n",
        "  LwLimit = Q1 - 1.5*I\n",
        "  UpLimit = Q3 + 1.5*I\n",
        "  ls = df.index[(df[ft] < LwLimit) | (df[ft] > UpLimit)]\n",
        "  return ls\n",
        "\n",
        "def remove(df, ls):\n",
        "  ls = sorted(set(ls))\n",
        "  df = df.drop(ls)\n",
        "  return df\n",
        "\n",
        "def apply_remotion(dataset):\n",
        "  remover = []\n",
        "  for i in dataset:\n",
        "    remover.extend(outliers(dataset, i))\n",
        "\n",
        "  dataset_cleaned = remove(dataset, remover)\n",
        "  return dataset_cleaned\n",
        "\n",
        "dataset = apply_remotion(dataset)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lku_S_6bAQCM"
      },
      "outputs": [],
      "source": [
        "'''plt.rcParams[\"figure.figsize\"] = (20,6)\n",
        "fig, axs = plt.subplots(1, 3)\n",
        "\n",
        "i=0\n",
        "for col in dataset:\n",
        "  axs[i].set_title(col)\n",
        "  axs[i].boxplot(dataset[col])\n",
        "  i +=1\n",
        "\n",
        "plt.show()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZs-l_hej1B_"
      },
      "source": [
        "Aqui, decidimos manter os outliers, visto que eles s√£o poucos e temos poucos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW2IYPpiAmJN"
      },
      "source": [
        "##2.4. An√°lise bivariada / correla√ß√µes entre os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7lcfnwIAztk"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(dataset)\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 10), sharey=True, constrained_layout=True)\n",
        "fig.suptitle('Compara√ß√£o entre os m√©todos')\n",
        "\n",
        "#Spearman\n",
        "ax = sns.heatmap(dataset.corr(method='spearman'), annot=True, vmin=-1, vmax=1, ax = axes[0,0])\n",
        "axes[0,0].set_title(\"Spearman\")\n",
        "bottom, top = ax.get_ylim()\n",
        "\n",
        "#Pearson\n",
        "ax = sns.heatmap(dataset.corr(method='pearson'), annot=True, vmin=-1, vmax=1, ax = axes[0,1])\n",
        "axes[0,1].set_title(\"Pearson\")\n",
        "bottom, top = ax.get_ylim()\n",
        "\n",
        "#Kendall\n",
        "ax = sns.heatmap(dataset.corr(method='kendall'), annot=True, vmin=-1, vmax=1, ax = axes[1,0])\n",
        "bottom, top = ax.get_ylim()\n",
        "axes[1,0].set_title(\"Kendall\")\n",
        "\n",
        "#Sem transforma√ß√£o\n",
        "ax = sns.heatmap(dataset.corr(), annot=True, vmin=-1, vmax=1, ax = axes[1,1])\n",
        "axes[1,1].set_title(\"Sem transforma√ß√£o\")\n",
        "bottom, top = ax.get_ylim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXbT7dwu1sIx"
      },
      "source": [
        "Aqui a an√°lise √© semelhante √† que fizemos no in√≠cio: B2 e B3 s√£o altamente correlacionados e, mesmo usando m√©todos n√£o lineares de correla√ß√£o (Pearson, Kendall e Spearman) a correla√ß√£o deles com P √© baixa. Al√©m disso, a correla√ß√£o de spearman da vari√°vel B3/B2 com a vari√°vel P √© moderada. Isso nos leva √†s seguintes conclus√µes.\n",
        "\n",
        "\n",
        "\n",
        "*   A informa√ß√£o das vari√°veis B2 e B3 √©, em grande parte, reduntante (como j√° vimos anteriormente);\n",
        "*   Como n√£o existe nenhuma correla√ß√£o forte com P, dificilmente m√©todos lineares de regress√£o ter√£o bom desempenho ao tentar predizer P a partir de B2, B3 e B3/B2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYv2tPFHFGTn"
      },
      "outputs": [],
      "source": [
        "b2 = np.array(dataset[dataset.columns[1]])\n",
        "b3 = np.array(dataset[dataset.columns[2]])\n",
        "\n",
        "b2 = (b2-b2.min())/((b2.max()-b2.min()))\n",
        "b3 = (b3-b3.min())/((b3.max()-b3.min()))\n",
        "\n",
        "b3 = [x for _, x in sorted(zip(b2, b3))]\n",
        "b2 = sorted(b2)\n",
        "\n",
        "plt.scatter(range(len(b2)), b2)\n",
        "plt.scatter(range(len(b2)), b3)\n",
        "plt.title('B2 x B3 comparison')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Samples')\n",
        "plt.legend(['B2', 'B3'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqMNmk6R2_QK"
      },
      "source": [
        "Aqui, fizemos esse plot apenas para mostrar que B2 e B3 continuam altamente correlacionadas, mesmo ap√≥s as transforma√ß√µes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsYMZUjqQMPN"
      },
      "source": [
        "#3. Algoritmos de regress√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXSLNuYj-s1Y"
      },
      "source": [
        "Dividiremos o conjunto de dados em treino e teste. Deixamos o conjunto de teste com apenas 15% das amostras, pois, como temos poucos dados, queremos maximizar o n√∫mero de amostras de treinamento. Por esse mesmo motivo, optamos por usar cross validation ao inv√©s de criar um conjunto de valida√ß√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2xeX6GV6s_6"
      },
      "outputs": [],
      "source": [
        "x_tr, x_te, y_tr, y_te = train_test_split(dataset[dataset.columns[1:]], dataset[dataset.columns[0]], test_size=0.15, random_state=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auqRnPyk7S8i"
      },
      "outputs": [],
      "source": [
        "def plot_results(y_pred, y_tes):\n",
        "  plt.scatter(range(len(y_pred)), y_pred, c='r')\n",
        "  plt.plot(range(len(y_tes)), y_tes, linestyle=\"-\", marker=\"o\", label=\"Expenses\")\n",
        "  plt.title('Model performance - test set')\n",
        "  plt.ylabel('P medido')\n",
        "  plt.xlabel('Sample')\n",
        "  plt.legend(['predicted', 'real'], loc='upper left')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY3wI9A19MX9"
      },
      "source": [
        "Antes de implementarmos os modelos de regress√£o, gostaria de deixar claro que usaremos duas m√©tricas principais para avaliar o desempenho de nossos modelos:\n",
        "\n",
        "\n",
        "\n",
        "*   Erro percentual m√©dio absoluto (mean absolute percentage error):\n",
        "\\begin{equation}\n",
        "  MAPE = \\frac{1}{n} \\sum_{i = 1}^{n} \\frac{|y_i - p_i|}{y_i}\n",
        "\\end{equation}\n",
        "\n",
        "*   Erro m√©dio absoluto (mean absolute error):\n",
        "\\begin{equation}\n",
        "  MAE = \\frac{1}{n} \\sum_{i = 1}^{n} |y_i - p_i|\n",
        "\\end{equation}\n",
        "\n",
        "*   $R^2$ score:\n",
        "\\begin{equation}\n",
        "  R^2 = NSE = 1 - \\frac{\\sum_{i=1}^{N} (y_i - p_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y_i})^2}\n",
        "\\end{equation}\n",
        "\n",
        "Onde, $y_i$ √© o valor real,  $p_i$ √© o valor predito pelo modelo testado e $\\bar{y_i}$ √© a m√©dia dos valores de $y_i$, $\\forall i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Fbf6Qc6WWb"
      },
      "source": [
        "##3.1. Apenas ignore esta parte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGxt7dqGYIEa"
      },
      "outputs": [],
      "source": [
        "def normalization(data, apply_log=False, scale=True, exponential=False):\n",
        "  df = data.drop('P medido (mg/L)', axis=1)\n",
        "\n",
        "  if apply_log:\n",
        "    df = df.astype(float)\n",
        "    df = np.log(df)\n",
        "\n",
        "  if exponential:\n",
        "    df = df.astype(float)\n",
        "    df.applymap(np.exp)\n",
        "\n",
        "  if scale:\n",
        "    for col in df.columns:\n",
        "      df[col] = df[col]/df[col].max()\n",
        "\n",
        "    targ = 'P medido (mg/L)'\n",
        "    data[targ] = data[targ]/data[targ].max()\n",
        "\n",
        "  df_norm = pd.concat((df, data['P medido (mg/L)']), 1)\n",
        "\n",
        "  return df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5kFtTNRXcbq"
      },
      "outputs": [],
      "source": [
        "'''pentecoste = normalization(pentecoste.copy(), apply_log=False, exponential=True, scale=False)\n",
        "acarape = normalization(acarape.copy(), apply_log=False, exponential=True, scale=False)\n",
        "aracoiaba = normalization(aracoiaba.copy(), apply_log=False, exponential=True, scale=False)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orXf89XfZ0cU"
      },
      "outputs": [],
      "source": [
        "'''x_tr, y_tr = np.array(pentecoste[pentecoste.columns[0:2]]), np.array(pentecoste[\"P medido (mg/L)\"])\n",
        "x_va, y_va = np.array(acarape[acarape.columns[0:2]]), np.array(acarape[\"P medido (mg/L)\"])\n",
        "x_te, y_te = np.array(aracoiaba[aracoiaba.columns[0:2]]), np.array(aracoiaba[\"P medido (mg/L)\"])\n",
        "\n",
        "x_tr = np.concatenate([x_tr, x_va])\n",
        "y_tr = np.concatenate([y_tr, y_va])\n",
        "del x_va, y_va\n",
        "\n",
        "x_tr = x_tr.astype('float64')\n",
        "y_tr = y_tr.astype('float64')\n",
        "x_te = x_te.astype('float64')\n",
        "y_te = y_te.astype('float64')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ3CgaJJ7eEt"
      },
      "source": [
        "##3.2. M√©todos Lineares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EniY3Uk_VcA"
      },
      "outputs": [],
      "source": [
        "import sklearn.model_selection as skms\n",
        "import sklearn.linear_model as sklm\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
        "from sklearn.metrics import make_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EmVz304_CFT"
      },
      "outputs": [],
      "source": [
        "def train_test_model(model, x_tr, y_tr, x_te, y_te, folds=5):\n",
        "  kf = KFold(n_splits = folds, random_state = 1, shuffle=True)\n",
        "  scores = cross_validate(model, x_tr, y_tr, cv=kf,\n",
        "                        scoring = ('neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'r2'))\n",
        "\n",
        "  mae = np.mean(scores['test_neg_mean_absolute_error']*(-1))\n",
        "  std_mae = np.std(scores['test_neg_mean_absolute_error']*(-1))\n",
        "\n",
        "  mape = np.mean(scores['test_neg_mean_absolute_percentage_error']*(-1))\n",
        "  std_mpae = np.std(scores['test_neg_mean_absolute_percentage_error']*(-1))\n",
        "\n",
        "  r2 = np.mean(scores['test_r2'])\n",
        "  std_r2 = np.std(scores['test_r2'])\n",
        "\n",
        "  print('--------------------Valida√ß√£o Cruazada-----------------------')\n",
        "  print(\"M√©dia dos valore de MAE: \" + str(mae))\n",
        "  print(\"Desvio padr√£o dos valore de MAE: \" + str(std_mae) + \"\\n\")\n",
        "\n",
        "  print(\"M√©dia dos valore de MAPE: \" + str(mape))\n",
        "  print(\"Desvio padr√£o dos valore de MAPE: \" + str(std_mpae) + \"\\n\")\n",
        "\n",
        "  print(\"M√©dia dos valore de R2: \" + str(r2))\n",
        "  print(\"Desvio padr√£o dos valore de R2: \" + str(std_r2) + \"\\n\")\n",
        "\n",
        "  print('--------------------Teste-----------------------')\n",
        "  y_pred = model.predict(x_te)\n",
        "  print(\"MAE: \" + str(mean_absolute_error(y_te, y_pred)))\n",
        "  print(\"MAPE: \" + str(mean_absolute_percentage_error(y_te, y_pred)))\n",
        "  print(\"R2: \" + str(r2_score(y_te, y_pred)))\n",
        "\n",
        "  plot_results(model.predict(x_te), y_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAf-pSZ8QCgp"
      },
      "source": [
        "###3.2.1. Regress√£o Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGDCh4HyMUjb"
      },
      "outputs": [],
      "source": [
        "linear_regressor = sklm.LinearRegression()\n",
        "linear_regressor.fit(x_tr, y_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Fq2AosRSz-"
      },
      "source": [
        "**Teste com 5-Folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pr3hJksND4l"
      },
      "outputs": [],
      "source": [
        "train_test_model(linear_regressor, x_tr, y_tr, x_te, y_te, folds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyMOF5ytRXGH"
      },
      "source": [
        "**Teste com 10-Folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy_jxTXRPED_"
      },
      "outputs": [],
      "source": [
        "train_test_model(linear_regressor, x_tr, y_tr, x_te, y_te, folds=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6v-t_cq8Xa3"
      },
      "outputs": [],
      "source": [
        "print(\"Coeficientes da regress√£o linear: \" + str(linear_regressor.coef_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNMzCTGlQZOD"
      },
      "source": [
        "###3.2.2. Rgress√£o Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0nltpmPVEju"
      },
      "outputs": [],
      "source": [
        "ray.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXKh8KYvQnV6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7f-PRRERGO5"
      },
      "outputs": [],
      "source": [
        "# definindo espa√ßo de busca para os lambdas\n",
        "lambdas = np.linspace(0, 100, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEJLhk6oU8be"
      },
      "source": [
        "**Teste com 5-Folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBCyKPLyQAbB"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "\n",
        "def test_lambda(lamb):\n",
        "  # aplicando a regress√£o\n",
        "  ridge = sklm.Ridge(alpha = lamb)\n",
        "  ridge.fit(x_tr, y_tr)             # Fit a ridge regression on the training data\n",
        "\n",
        "  scores = skms.cross_validate(ridge, x_tr, y_tr, cv=5,\n",
        "                               scoring=('r2', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error'))\n",
        "\n",
        "  mae = np.mean(scores['test_neg_mean_absolute_error']*(-1))\n",
        "  mape = np.mean(scores['test_neg_mean_absolute_percentage_error']*(-1))\n",
        "  r2 = np.mean(scores['test_r2'])\n",
        "\n",
        "  return [mae, mape, r2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0B0IR59VDW5"
      },
      "outputs": [],
      "source": [
        "result_values = ray.get([test_lambda.remote(i) for i in lambdas])\n",
        "scores_MAE = [result[0] for result in result_values]\n",
        "scores_MAPE = [result[1] for result in result_values]\n",
        "scores_R2 = [result[2] for result in result_values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG2Hyu-zRbSY"
      },
      "outputs": [],
      "source": [
        "best_index = scores_MAE.index(min(scores_MAE))\n",
        "\n",
        "print(\"Lambda:\" + str(lambdas[best_index]))\n",
        "print(\"Best MAE: \" + str(scores_MAE[best_index]))\n",
        "print(\"Best MAPE: \" + str(scores_MAPE[best_index]))\n",
        "print(\"R2: \" + str(scores_R2[best_index]))\n",
        "\n",
        "ridge = sklm.Ridge(alpha = lambdas[best_index])\n",
        "ridge.fit(x_tr, y_tr)\n",
        "plot_results(ridge.predict(x_te), y_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ovY-2X6XJrI"
      },
      "source": [
        "**Teste com 10-folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K4QdOxtXQTG"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "\n",
        "def test_lambda(lamb):\n",
        "  # aplicando a regress√£o\n",
        "  ridge = sklm.Ridge(alpha = lamb)\n",
        "  ridge.fit(x_tr, y_tr)             # Fit a ridge regression on the training data\n",
        "\n",
        "  scores = skms.cross_validate(ridge, x_tr, y_tr, cv=10,\n",
        "                               scoring=('r2', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error'))\n",
        "\n",
        "  mae = np.mean(scores['test_neg_mean_absolute_error']*(-1))\n",
        "  mape = np.mean(scores['test_neg_mean_absolute_percentage_error']*(-1))\n",
        "  r2 = np.mean(scores['test_r2'])\n",
        "\n",
        "  return [mae, mape, r2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RoUq55aXQTH"
      },
      "outputs": [],
      "source": [
        "result_values = ray.get([test_lambda.remote(i) for i in lambdas])\n",
        "scores_MAE = [result[0] for result in result_values]\n",
        "scores_MAPE = [result[1] for result in result_values]\n",
        "scores_R2 = [result[2] for result in result_values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8-8lWb_XQTI"
      },
      "outputs": [],
      "source": [
        "best_index = scores_MAE.index(min(scores_MAE))\n",
        "\n",
        "print(\"Lambda:\" + str(lambdas[best_index]))\n",
        "print(\"Best MAE: \" + str(scores_MAE[best_index]))\n",
        "print(\"Best MAPE: \" + str(scores_MAPE[best_index]))\n",
        "print(\"R2: \" + str(scores_R2[best_index]))\n",
        "\n",
        "ridge = sklm.Ridge(alpha = lambdas[best_index])\n",
        "ridge.fit(x_tr, y_tr)\n",
        "plot_results(ridge.predict(x_te), y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEGxfIpiwfxI"
      },
      "outputs": [],
      "source": [
        "y_pred = ridge.predict(x_te)\n",
        "print('-----------Teste-------------')\n",
        "print(\"MAE: \" + str(mean_absolute_error(y_te, y_pred)))\n",
        "print(\"MAPE: \" + str(mean_absolute_percentage_error(y_te, y_pred)))\n",
        "print(\"R2: \" + str(r2_score(y_te, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXycbyxHD_Gs"
      },
      "source": [
        "##3.3 M√©todos n√£o lineares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap1hd-gXWcjy"
      },
      "source": [
        "###3.3.1. SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXGKe0x_W0oz"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h07bCh1XX_A"
      },
      "outputs": [],
      "source": [
        "svm_regressor = svm.SVR()\n",
        "svm_regressor.fit(x_tr, y_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH8wKPtpJmgs"
      },
      "source": [
        "**Teste com 5-Folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb78mm80YvzJ"
      },
      "outputs": [],
      "source": [
        "train_test_model(svm_regressor, x_tr, y_tr, x_te, y_te, folds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qttgprVlJpsh"
      },
      "source": [
        "**Teste com 10-Folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBgRAvW-JY0K"
      },
      "outputs": [],
      "source": [
        "train_test_model(svm_regressor, x_tr, y_tr, x_te, y_te, folds=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKHWBLN0iuoB"
      },
      "source": [
        "###3.3.2. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nclCiyZsiuoC"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor as rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNVEZJShiuoD"
      },
      "outputs": [],
      "source": [
        "rf_regressor = rf(n_estimators=1000, random_state=1)\n",
        "rf_regressor.fit(x_tr, y_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g48l1W5iuoE"
      },
      "source": [
        "**Teste com 5-Folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE_wcjpNiuoE"
      },
      "outputs": [],
      "source": [
        "train_test_model(rf_regressor, x_tr, y_tr, x_te, y_te, folds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXpVC3JZiuoF"
      },
      "source": [
        "**Teste com 10-Folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1Dmq8w5iuoG"
      },
      "outputs": [],
      "source": [
        "train_test_model(rf_regressor, x_tr, y_tr, x_te, y_te, folds=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EswgXu1CQT5I"
      },
      "source": [
        "###3.3.3. XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsQjgz5-WayY"
      },
      "source": [
        "**Uma brever explica√ß√£o de como treinaremos este modelo (o mesmo vale para o *multilayer perceptron* - MLP):**\n",
        "\n",
        "\n",
        "\n",
        "*   Utilizaremos um algoritmo de otimiza√ß√£o (otimiza√ß√£o Bayesiana) para otimizar os hiperpar√¢metros dos modelos. Basicamente, hiperpar√¢metros s√£o valores configur√°veis que podem alterar a forma como o modelo aprende, por exemplo, em uma rede neural multicamadas (MLP) os hiperpar√¢metros podem ser o n√∫mero de camadas, o n√∫mero de nur√¥nios em cada camada, o n√∫mero de amostras passadas por vez para o modelo durante o treinamento, etc\n",
        "\n",
        "*   Salvaremos os melhores hiperpar√¢metros obtidos e o melhor modelo obtido na etapa de cross validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TJpymgpJGG1"
      },
      "outputs": [],
      "source": [
        "best = np.inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s1Vi6cD1TAn"
      },
      "outputs": [],
      "source": [
        "def r2_eval(y_pred, dtrain):\n",
        "    y_true = dtrain.get_label()\n",
        "    return 'r2', r2_score(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjrTY_L8swGm"
      },
      "outputs": [],
      "source": [
        "def cross_validation_xgb(x_train, y_train, params, folds=5):\n",
        "\n",
        "  global best\n",
        "\n",
        "  kf = KFold(n_splits=folds, random_state=1, shuffle=True)\n",
        "  x_tr = np.array(x_train)\n",
        "  y_tr = np.array(y_train)\n",
        "  histories = []\n",
        "  models = []\n",
        "  results = []\n",
        "\n",
        "  for train_index, test_index in kf.split(x_tr):\n",
        "    xtr, xva = x_tr[train_index], x_tr[test_index]\n",
        "    ytr, yva = y_tr[train_index], y_tr[test_index]\n",
        "\n",
        "    dtrain = xgboost.DMatrix(xtr, ytr)\n",
        "    dval = xgboost.DMatrix(xva, yva)\n",
        "\n",
        "    history = {}\n",
        "    model = xgboost.train(params=params, dtrain=dtrain, num_boost_round=800,\n",
        "                          evals=[(dtrain, 'train'), (dval, 'val')],\n",
        "                          early_stopping_rounds=15, evals_result=history,\n",
        "                          verbose_eval=False)\n",
        "\n",
        "    r2 = r2_score(yva, model.predict(dval))\n",
        "    mae = mean_absolute_error(yva, model.predict(dval))\n",
        "    mape = mean_absolute_percentage_error(yva, model.predict(dval))\n",
        "    result = {'r2':r2, 'mae':mae, 'mape':mape}\n",
        "\n",
        "    histories.append(history)\n",
        "    models.append(model)\n",
        "    results.append(result)\n",
        "\n",
        "    if mae < best:\n",
        "      best = mae\n",
        "      model.save_model(\"best_xgb.model\")\n",
        "\n",
        "    del model\n",
        "\n",
        "  return histories, models, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc2Z2uotlQvD"
      },
      "outputs": [],
      "source": [
        "def model_optimize(learning_rate, max_depth, min_c_weight, gamma, subsample,\n",
        "                   colsample_bytree):\n",
        "\n",
        "  max_depth = round(max_depth)\n",
        "\n",
        "  params = {'max_depth':max_depth, 'min_child_weight':min_c_weight, 'gamma':gamma,\n",
        "            'subsample':subsample, 'colsample_bytree':colsample_bytree,\n",
        "            'learning_rate':learning_rate, 'custom_metric':r2_eval,\n",
        "            'eval_metric':[\"mae\", \"mape\", \"logloss\"]}\n",
        "\n",
        "\n",
        "  histories, models, results = cross_validation_xgb(x_tr, y_tr, params, folds=5)\n",
        "\n",
        "  # pdb.set_trace()\n",
        "\n",
        "  # model = load_model('_pesos_lstm1.h5')\n",
        "  # os.remove(\"_pesos_lstm1.h5\")\n",
        "\n",
        "  plt.rcParams['figure.figsize']=(20,5)\n",
        "  plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "  history_mae = np.array([h['train']['mae'] for h in histories])\n",
        "  history_vmae = np.array([h['val']['mae'] for h in histories])\n",
        "\n",
        "  plt.plot(np.mean(history_mae, axis=0))\n",
        "  plt.plot(np.mean(history_vmae, axis=0))\n",
        "  plt.title('model mae')\n",
        "  plt.ylabel('mae')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  gc.collect()\n",
        "  scores = np.array([r['r2'] for r in results])\n",
        "  return scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GMaA5oUlTfS"
      },
      "outputs": [],
      "source": [
        "pbounds = {'max_depth': (3, 100),\n",
        "           'learning_rate': (0.001, 1),\n",
        "           'min_c_weight': (1, 100),\n",
        "           'gamma': (0, 0.3),\n",
        "           'subsample': (0.1, 1),\n",
        "           'colsample_bytree': (0.1, 1),\n",
        "        }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=model_optimize,\n",
        "    pbounds=pbounds,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# load_logs(optimizer, logs=[\"./logs.json\"])\n",
        "# logger = JSONLogger(path=\"./logs.json\")\n",
        "# optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
        "optimizer.maximize(init_points=50, n_iter=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qkOnkPaYGVR"
      },
      "source": [
        "**Teste de um modelo \"configurado\" com os melhores hiperpar√¢metros obtidos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meQ36Eu5xRKy"
      },
      "outputs": [],
      "source": [
        "best_hpp = optimizer.max['params']\n",
        "best_hpp['max_depth'] = round(best_hpp['max_depth'])\n",
        "best_hpp['eval_metric'] = [\"mae\", \"mape\", \"logloss\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7TL31mZxeIu"
      },
      "outputs": [],
      "source": [
        "best_xgb = XGBRegressor(n_estimators=100, **best_hpp)\n",
        "best_xgb.fit(x_tr, y_tr, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEkrLn4Vykzp"
      },
      "outputs": [],
      "source": [
        "y_pred = best_xgb.predict(x_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa9VSYBOyv6Y"
      },
      "outputs": [],
      "source": [
        "print(\"MAE: \" + str(mean_absolute_error(y_te, y_pred)))\n",
        "print(\"MAPE: \" + str(mean_absolute_percentage_error(y_te, y_pred)))\n",
        "print(\"R2: \" + str(r2_score(y_te, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8UhaFCjYhYK"
      },
      "source": [
        "**Teste do melhor modelo obtido na cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7oN7yHMUmdL"
      },
      "outputs": [],
      "source": [
        "best_xgb = XGBRegressor()\n",
        "best_xgb.load_model('best_xgb.model')\n",
        "\n",
        "y_pred = best_xgb.predict(x_te)\n",
        "\n",
        "print(\"MAE: \" + str(mean_absolute_error(y_te, y_pred)))\n",
        "print(\"MAPE: \" + str(mean_absolute_percentage_error(y_te, y_pred)))\n",
        "print(\"R2: \" + str(r2_score(y_te, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCiWOMe8n-Y2"
      },
      "source": [
        "###3.3.4 MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ablEMpdVj5I"
      },
      "outputs": [],
      "source": [
        "best = np.inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hylHdSbQ2YQr"
      },
      "outputs": [],
      "source": [
        "x_tr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_smFIkgoBNa"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, Adadelta, Adamax, RMSprop\n",
        "from tensorflow.keras.layers import PReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWofeTkmP9dq"
      },
      "outputs": [],
      "source": [
        "def cross_validation(x_train, y_train, dense_layers, neurons_1, neurons_2, neurons_3, optimizer,\n",
        "                     learning_rate, dropout, folds=5, batch_size=32, callbacks=None):\n",
        "\n",
        "  global best\n",
        "\n",
        "  kf = KFold(n_splits=folds, random_state=1, shuffle=True)\n",
        "  x_tr = np.array(x_train)\n",
        "  y_tr = np.array(y_train)\n",
        "  histories = []\n",
        "  models = []\n",
        "  results = []\n",
        "\n",
        "  for train_index, test_index in kf.split(x_tr):\n",
        "    xtr, xva = x_tr[train_index], x_tr[test_index]\n",
        "    ytr, yva = y_tr[train_index], y_tr[test_index]\n",
        "\n",
        "    model = ann(neurons_1, neurons_2, neurons_3, dense_layers, dropout, optimizer,\n",
        "                learning_rate, xtr)\n",
        "\n",
        "    history = model.fit(xtr, ytr, validation_data = (xva, yva),\n",
        "                epochs=800, batch_size = batch_size, callbacks=callbacks,\n",
        "                verbose=0)\n",
        "\n",
        "    r2 = r2_score(yva, model.predict(xva))\n",
        "    mae = mean_absolute_error(yva, model.predict(xva))\n",
        "    mape = mean_absolute_percentage_error(yva, model.predict(xva))\n",
        "    result = {'r2':r2, 'mae':mae, 'mape':mape}\n",
        "\n",
        "    histories.append(history)\n",
        "    models.append(model)\n",
        "    results.append(result)\n",
        "\n",
        "    if mae < best:\n",
        "      best = mae\n",
        "      model.save(\"best_ann.keras\")\n",
        "\n",
        "  return histories, models, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhIFzv23tnSu"
      },
      "outputs": [],
      "source": [
        "def ann(neurons_1, neurons_2, neurons_3, dense_layers, dropout, optimizer, learning_rate, xtr):\n",
        "\n",
        "  neurons = [neurons_1, neurons_2, neurons_3]\n",
        "\n",
        "  ann = Sequential()\n",
        "\n",
        "  for i in range(0, dense_layers):\n",
        "    if i == 0:\n",
        "      ann.add(Dense(neurons[i], activation=\"relu\", input_shape=(None, xtr.shape[1])))\n",
        "      ann.add(Dropout(dropout))\n",
        "    else:\n",
        "      ann.add(Dense(neurons[i], activation=\"relu\"))\n",
        "      ann.add(Dropout(dropout))\n",
        "\n",
        "  ann.add(Dense(1, activation=None))\n",
        "  ann.compile(optimizer = optimizer(learning_rate=learning_rate), loss='mse', metrics = ['mae', tensorflow.keras.metrics.R2Score()])\n",
        "  return ann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh0vxx8YxwXx"
      },
      "outputs": [],
      "source": [
        "def evaluate_network(dense_layers, neurons_1, neurons_2, neurons_3, optimizer, batch_size, learning_rate, dropout):\n",
        "\n",
        "    dense_layers = round(dense_layers)\n",
        "    neurons_1 = round(neurons_1); neurons_2 = round(neurons_2); neurons_3 = round(neurons_3)\n",
        "    optimizer = round(optimizer)\n",
        "    batch_size = 2**round(batch_size)\n",
        "\n",
        "    optimizer_array = [Adam, Adadelta, Adamax, RMSprop]\n",
        "    optimizer_val = optimizer_array[optimizer]\n",
        "\n",
        "    es = EarlyStopping(monitor=\"val_mae\", mode='min', verbose=0, patience=15)\n",
        "    # checkpoint = ModelCheckpoint('_pesos_mlp.h5', monitor=\"val_mae\", verbose=0,\n",
        "    #                                   save_best_only=True, mode='min')\n",
        "\n",
        "    histories, models, results = cross_validation(x_tr, y_tr,dense_layers, neurons_1,\n",
        "                                         neurons_2, neurons_3, optimizer_val,\n",
        "                                         learning_rate, dropout,folds=5,\n",
        "                                         batch_size=batch_size, callbacks=es)\n",
        "\n",
        "    plt.rcParams['figure.figsize']=(20,5)\n",
        "    plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "    history_mae = np.array([h.history['mae'] for h in histories])\n",
        "    history_vmae = np.array([h.history['val_mae'] for h in histories])\n",
        "\n",
        "    plt.plot(np.mean(history_mae, axis=0))\n",
        "    plt.plot(np.mean(history_vmae, axis=0))\n",
        "    plt.title('model mae')\n",
        "    plt.ylabel('mae')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    #calculating score\n",
        "    score = np.mean([r['r2'] for r in results])\n",
        "\n",
        "    gc.collect()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsxd6yzgztHd"
      },
      "outputs": [],
      "source": [
        "pbounds = {'dense_layers': (1, 3),\n",
        "           'neurons_1': (1, 16),\n",
        "           'neurons_2': (1, 8),\n",
        "           'neurons_3': (1, 4),\n",
        "           'optimizer': (0, 3),\n",
        "           'batch_size': (5, 7),\n",
        "           'learning_rate': (0.0001, 1),\n",
        "           'dropout': (0, 0.5)\n",
        "        }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_network,\n",
        "    pbounds=pbounds,\n",
        "    random_state=2\n",
        ")\n",
        "\n",
        "# load_logs(optimizer, logs=[\"./logs.json\"])\n",
        "# logger = JSONLogger(path=\"./logs.json\")\n",
        "# optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
        "optimizer.maximize(init_points=50, n_iter=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DipIwm4VY0EY"
      },
      "source": [
        "**Teste de um modelo \"configurado\" com os melhores hiperpar√¢metros obtidos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feiyFcUHRhun"
      },
      "outputs": [],
      "source": [
        "params = optimizer.max['params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJO4z-XMAwSE"
      },
      "outputs": [],
      "source": [
        "params['dense_layers'] = round(params['dense_layers'])\n",
        "params['neurons_1'] = round(params['neurons_1'])\n",
        "params['neurons_2'] = round(params['neurons_2'])\n",
        "params['neurons_3'] = round(params['neurons_3'])\n",
        "params['batch_size'] = 2**round(params['batch_size'])\n",
        "\n",
        "optimizer_array = [Adam, Adadelta, Adamax, RMSprop]\n",
        "params['optimizer'] = optimizer_array[round(params['optimizer'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmJu5hLpCuUP"
      },
      "outputs": [],
      "source": [
        "model = ann(params['neurons_1'], params['neurons_2'], params['neurons_3'],\n",
        "            params['dense_layers'], params['dropout'], params['optimizer'],\n",
        "            params['learning_rate'], x_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_ppaWkHDS5i"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor=\"mae\", mode='min', verbose=0, patience=15)\n",
        "model.fit(x_tr, y_tr, epochs=800, batch_size = params['batch_size'], callbacks=es, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSSk-eLEULf"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x_te)\n",
        "\n",
        "print(\"MAE: \" + str(mean_absolute_error(y_te, y_pred)))\n",
        "print(\"MAPE: \" + str(mean_absolute_percentage_error(y_te, y_pred)))\n",
        "print(\"R2: \" + str(r2_score(y_te, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq7svzHOZIpa"
      },
      "source": [
        "**Teste do melhor modelo obtido na cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3nf_6FoZIpb"
      },
      "outputs": [],
      "source": [
        "best_ann = keras.models.load_model('best_ann.keras')\n",
        "\n",
        "y_pred = best_ann.predict(x_te)\n",
        "\n",
        "print(\"MAE: \" + str(mean_absolute_error(y_te, y_pred)))\n",
        "print(\"MAPE: \" + str(mean_absolute_percentage_error(y_te, y_pred)))\n",
        "print(\"R2: \" + str(r2_score(y_te, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzICwRhLqMYQ"
      },
      "source": [
        "#4. Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM2ys1yqJ65-"
      },
      "outputs": [],
      "source": [
        "MAE: 0.4152660683467405\n",
        "MAPE: 0.20966133201606202\n",
        "R2: -0.0248688355671991"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X8DpK17qQ5n"
      },
      "source": [
        "O melhor resultado foi obtido pelo SVM, com um MAE de 0.415, MAPE de 0.21 e $R^2$ de -0.025.\n",
        "\n",
        "Usando o valor de $\\lambda = 0.05245$ obtido a partir da transforma√ß√£o Box-cox. Calcualndo, portanto o erro em rela√ß√£o ao 'P medido', temos:\n",
        "\n",
        "\\begin{gather}\n",
        "      y(\\lambda) = \\frac{y^\\lambda - 1}{\\lambda}\n",
        "\\end{gather}\n",
        "\n",
        "Substituindo os valores de $\\lambda$ e y($\\lambda$) (MAE), temos\n",
        "\n",
        "\\begin{gather}\n",
        "      0.415 = \\frac{P^{0.05245} - 1}{0.05245}\\\\ \\\\\n",
        "      0.415 \\cdot 0.05245 + 1 = P^{0.05245} \\\\ \\\\\n",
        "      P = \\log_{ \\; 0.05245}(0.415 \\cdot 0.05245 + 1)\n",
        "\\end{gather}      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLPc6uT2uPl9"
      },
      "outputs": [],
      "source": [
        "math.log(0.415*0.05245 + 1, 0.05245)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO7bCDpA7GDE"
      },
      "source": [
        "Ou seja, o resultado acima indica que nosso modelo errou, em m√©dia, por aproximadamente $-0.0073$ mg/L da quantidade real de P medido do conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDh9EjIO5wYV"
      },
      "outputs": [],
      "source": [
        "svm_regressor.fit(x_tr, y_tr)\n",
        "y_pred = svm_regressor.predict(x_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr-fFH3-5zaU"
      },
      "outputs": [],
      "source": [
        "y_tes = y_te.tolist()\n",
        "for i in range(len(y_pred)):\n",
        "  y_pred[i] = math.log(y_pred[i]*0.05245 + 1, 0.05245)\n",
        "  y_tes[i] = math.log(y_tes[i]*0.05245 + 1, 0.05245)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twny1o_-7VyT"
      },
      "source": [
        "Transformamos os valores do conjunto de teste e os valores preditos pelo modelo de regress√£o para a unidade original (mg/L), ou seja para a unidade a anterior √† aplica√ß√£o do Box-cox. O gr√°fico abaixo mostra o desempenho do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXuLqJlb5VZr"
      },
      "outputs": [],
      "source": [
        "plot_results(y_pred, y_tes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mPHr238enK"
      },
      "source": [
        "#5. Conclus√µes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUj169RR81AF"
      },
      "source": [
        "\n",
        "\n",
        "*   Como n√£o conhe√ßo as caracter√≠sticas do problema, n√£o sei se os valores para os erros m√©dios obtidos s√£o aceit√°veis;\n",
        "\n",
        "*   Mesmo n√£o conhecendo a natureza do problema, recomendo fortemente a busca por novas vari√°veis. Como vimos, a informa√ß√£o das vari√°veis B2 e B3 √© redundante. Al√©m disso, essas vari√°veis parecem fornecer pouca informa√ß√£o sobre a vari√°vel P;\n",
        "\n",
        "*   Para termos mais confian√ßa nos resultados, seriam necess√°rios mais dados, pois, quanto menos dados de teste, menor nossa certeza na capacidade de generaliza√ß√£o do modelo;\n",
        "\n",
        "*   Por fim, a sugest√£o de adicionar a m√©trica B3/B2 parece ter melhorado os resultados.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDvlvisUV0j9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YugYfK9-V343"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM5zAiI4V5qc"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/CNN_Chagas/fosforo/Analise_fosforo.ipynb ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqlNmhaGV7Rp"
      },
      "outputs": [],
      "source": [
        "!jupyter nbconvert --to PDF \"Analise_fosforo.ipynb\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K_Fbf6Qc6WWb"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}